#!/bin/bash

# Simplified script to add lm_ prefixes to specified files
# Based on logic from patch-from-llama-cpp.sh but without risky operations

# Set project root directory
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"

# List of files to add lm_ prefix to
files_add_lm_prefix=(
  "$PROJECT_ROOT/lib/llama_cpp/llama-impl.h"
  "$PROJECT_ROOT/lib/llama_cpp/llama-impl.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/llama-vocab.h"
  "$PROJECT_ROOT/lib/llama_cpp/llama-vocab.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/llama-grammar.h"
  "$PROJECT_ROOT/lib/llama_cpp/llama-grammar.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/llama-sampling.h"
  "$PROJECT_ROOT/lib/llama_cpp/llama-sampling.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/llama-adapter.h"
  "$PROJECT_ROOT/lib/llama_cpp/llama-adapter.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/llama-arch.h"
  "$PROJECT_ROOT/lib/llama_cpp/llama-arch.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/llama-batch.h"
  "$PROJECT_ROOT/lib/llama_cpp/llama-batch.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/llama-chat.h"
  "$PROJECT_ROOT/lib/llama_cpp/llama-chat.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/llama-context.h"
  "$PROJECT_ROOT/lib/llama_cpp/llama-context.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/llama-kv-cache.h"
  "$PROJECT_ROOT/lib/llama_cpp/llama-kv-cache.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/llama-kv-cache-iswa.h"
  "$PROJECT_ROOT/lib/llama_cpp/llama-model-loader.h"
  "$PROJECT_ROOT/lib/llama_cpp/llama-model-loader.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/llama-model-saver.h"
  "$PROJECT_ROOT/lib/llama_cpp/llama-model-saver.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/llama-model.h"
  "$PROJECT_ROOT/lib/llama_cpp/llama-model.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/llama-mmap.h"
  "$PROJECT_ROOT/lib/llama_cpp/llama-mmap.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/llama-hparams.h"
  "$PROJECT_ROOT/lib/llama_cpp/llama-hparams.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/llama-cparams.h"
  "$PROJECT_ROOT/lib/llama_cpp/llama-cparams.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/llama-graph.h"
  "$PROJECT_ROOT/lib/llama_cpp/llama-graph.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/llama-io.h"
  "$PROJECT_ROOT/lib/llama_cpp/llama-io.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/llama-memory.h"
  "$PROJECT_ROOT/lib/llama_cpp/llama-memory.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/llama-memory-hybrid.h"
  "$PROJECT_ROOT/lib/llama_cpp/llama-memory-recurrent.h"
  "$PROJECT_ROOT/lib/llama_cpp/log.h"
  "$PROJECT_ROOT/lib/llama_cpp/log.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/llama.h"
  "$PROJECT_ROOT/lib/llama_cpp/llama.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpp.h"
  "$PROJECT_ROOT/lib/llama_cpp/sampling.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/sampling.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/sgemm.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/sgemm.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/common.h"
  "$PROJECT_ROOT/lib/llama_cpp/common.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/json-schema-to-grammar.h"
  "$PROJECT_ROOT/lib/llama_cpp/json-schema-to-grammar.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/chat.h"
  "$PROJECT_ROOT/lib/llama_cpp/chat.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/base64.hpp"
  "$PROJECT_ROOT/lib/llama_cpp/chat-parser.h"
  "$PROJECT_ROOT/lib/llama_cpp/chat-parser.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/chat-peg-parser.h"
  "$PROJECT_ROOT/lib/llama_cpp/chat-peg-parser.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/chat-parser-xml-toolcall.h"
  "$PROJECT_ROOT/lib/llama_cpp/chat-parser-xml-toolcall.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/console.h"
  "$PROJECT_ROOT/lib/llama_cpp/console.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/arg.h"
  "$PROJECT_ROOT/lib/llama_cpp/arg.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/http.h"
  "$PROJECT_ROOT/lib/llama_cpp/download.h"
  "$PROJECT_ROOT/lib/llama_cpp/download.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/json-partial.h"
  "$PROJECT_ROOT/lib/llama_cpp/json-partial.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/json.hpp"
  "$PROJECT_ROOT/lib/llama_cpp/llguidance.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ngram-cache.h"
  "$PROJECT_ROOT/lib/llama_cpp/ngram-cache.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/preset.h"
  "$PROJECT_ROOT/lib/llama_cpp/preset.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/regex-partial.h"
  "$PROJECT_ROOT/lib/llama_cpp/regex-partial.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/speculative.h"
  "$PROJECT_ROOT/lib/llama_cpp/speculative.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/unicode.h"
  "$PROJECT_ROOT/lib/llama_cpp/unicode.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/unicode-data.h"
  "$PROJECT_ROOT/lib/llama_cpp/unicode-data.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/peg-parser.h"
  "$PROJECT_ROOT/lib/llama_cpp/peg-parser.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-common.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml.c"
  "$PROJECT_ROOT/lib/llama_cpp/gguf.h"
  "$PROJECT_ROOT/lib/llama_cpp/gguf.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-impl.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-impl.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-backend.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-backend.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-backend-reg.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cuda.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cuda.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-metal.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-metal.m"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-metal.metal"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-metal-device.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-metal-context.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-metal-common.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-metal-ops.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-metal-impl.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-quants.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-quants.c"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-alloc.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-alloc.c"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-aarch64.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-aarch64.c"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-opencl.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-opencl.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-vulkan.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-vulkan.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-kernels.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-kernels.c"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/kernels.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/kernels.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/ggml-cpu.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/ggml-cpu.c"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/ggml-cpu-impl.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/quants.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/traits.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/vec.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/simd-mappings.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/ggml-cpu-aarch64-quants.c"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/ggml-cpu-aarch64-feats.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/ggml-cpu-aarch64-repack.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/ops.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/ops.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/binary-ops.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/binary-ops.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/unary-ops.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/unary-ops.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/vec.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/ggml-cpu-quants.c"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/ggml-cpu-traits.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/arch-fallback.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/common.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/amx/amx.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/amx/amx.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/amx/mmq.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/amx/mmq.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/amx/common.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-opt.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-opt.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-backend-impl.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-threading.h"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-cpu/repack.h"
  "$PROJECT_ROOT/lib/llama_cpp/models/models.h"
  "$PROJECT_ROOT/lib/llama_cpp/models/clip.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/clip.h"
  "$PROJECT_ROOT/lib/llama_cpp/models/bert.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/bert.h"
  "$PROJECT_ROOT/lib/llama_cpp/models/whisper.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/whisper.h"
  "$PROJECT_ROOT/lib/llama_cpp/models/mistral.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/gemma.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/llama.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/mixtral.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/phi.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/gpt2.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/gpt_j.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/gpt_neox.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/gptq.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/llava.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/mpt.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/replit.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/starcoder.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/llama2.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/yi.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/wizard.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/chatglm.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/llama3.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/llama3-instruct.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/phi3.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/phi3-mini.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/qwen.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/qwen2.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/qwen2-impl.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/qwen2-impl.h"
  "$PROJECT_ROOT/lib/llama_cpp/models/qwen2-tokenizer.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/qwen2-tokenizer.h"
  "$PROJECT_ROOT/lib/llama_cpp/models/llava-clip.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/llava-clip.h"
  "$PROJECT_ROOT/lib/llama_cpp/models/llava-impl.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/llava-impl.h"
  "$PROJECT_ROOT/lib/llama_cpp/models/mixtral-impl.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/mixtral-impl.h"
  "$PROJECT_ROOT/lib/llama_cpp/models/starcoder2.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/t5.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/t5.h"
  "$PROJECT_ROOT/lib/llama_cpp/models/llama-tokenizer.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/llama-tokenizer.h"
  "$PROJECT_ROOT/lib/llama_cpp/models/clip-tokenizer.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/clip-tokenizer.h"
  "$PROJECT_ROOT/lib/llama_cpp/models/mistral-tokenizer.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/mistral-tokenizer.h"
  "$PROJECT_ROOT/lib/llama_cpp/models/whisper-tokenizer.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/whisper-tokenizer.h"
  "$PROJECT_ROOT/lib/llama_cpp/models/wavtokenizer-enc.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/wavtokenizer-dec.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/xverse.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/afmoe.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/apertus.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/arcee.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/arctic.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/arwkv7.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/baichuan.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/bailingmoe.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/bailingmoe2.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/bert.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/bitnet.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/bloom.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/chameleon.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/chatglm.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/codeshell.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/cogvlm.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/cohere2-iswa.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/command-r.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/dbrx.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/deci.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/deepseek.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/deepseek2.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/dots1.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/dream.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/ernie4-5-moe.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/ernie4-5.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/exaone.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/exaone4.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/falcon-h1.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/falcon.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/gemma-embedding.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/gemma.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/gemma2-iswa.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/gemma3.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/gemma3n-iswa.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/glm4-moe.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/glm4.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/gpt2.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/gptneox.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/granite-hybrid.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/granite.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/graph-context-mamba.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/grok.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/grovemoe.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/hunyuan-dense.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/hunyuan-moe.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/internlm2.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/jais.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/jamba.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/lfm2.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/llada-moe.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/llada.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/llama-iswa.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/llama.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/mamba.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/minicpm3.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/minimax-m2.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/mistral3.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/models.h"
  "$PROJECT_ROOT/lib/llama_cpp/models/modern-bert.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/mpt.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/nemotron-h.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/nemotron.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/neo-bert.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/olmo.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/olmo2.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/olmoe.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/openai-moe-iswa.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/openelm.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/orion.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/pangu-embedded.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/phi2.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/phi3.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/plamo.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/plamo2.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/plm.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/qwen.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/qwen2.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/qwen2moe.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/qwen2vl.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/qwen3.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/qwen3moe.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/qwen3next.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/qwen3vl-moe.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/qwen3vl.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/refact.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/rnd1.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/rwkv6-base.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/rwkv6.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/rwkv6qwen2.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/rwkv7-base.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/rwkv7.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/seed-oss.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/smallthinker.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/smollm3.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/stablelm.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/starcoder.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/t5-dec.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/t5-enc.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/models/wavtokenizer-dec.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/mtmd.h"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/mtmd-helper.h"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/mtmd-helper.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/mtmd.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/mtmd-audio.h"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/mtmd-audio.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/clip.h"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/clip.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/clip-impl.h"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/clip-graph.h"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/clip-model.h"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/models/models.h"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/models/clip.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/models/conformer.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/models/cogvlm.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/models/glm4v.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/models/internvl.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/models/kimivl.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/models/llama4.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/models/llava.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/models/minicpmv.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/models/pixtral.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/models/qwen2vl.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/models/qwen3vl.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/models/siglip.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/tools/mtmd/models/whisper-enc.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-metal.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-metal-device.m"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-metal-device.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-metal-context.m"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-metal-common.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-metal-ops.cpp"
  "$PROJECT_ROOT/lib/llama_cpp/ggml-metal.m"
)

echo "Adding lm_ prefixes to files..."

OS=$(uname)
for file in "${files_add_lm_prefix[@]}"; do
  # Check if file exists before processing
  if [ -f "$file" ]; then
    # Check if file already has mostly lm_ggml_ prefixes to avoid unnecessary processing
    # Count occurrences of prefixed and non-prefixed symbols
    
    # Using wc -l for clean integer counts
    lm_ggml_count=$(grep -o "lm_ggml_" "$file" 2>/dev/null | wc -l)
    lm_ggml_count=${lm_ggml_count:-0}
    
    all_ggml=$(grep -o "ggml_" "$file" 2>/dev/null | wc -l)
    all_ggml=${all_ggml:-0}
    
    ggml_count=$((all_ggml - lm_ggml_count))
    
    LM_GGML_count=$(grep -o "LM_GGML_" "$file" 2>/dev/null | wc -l)
    LM_GGML_count=${LM_GGML_count:-0}
    
    all_GGML=$(grep -o "GGML_" "$file" 2>/dev/null | wc -l)
    all_GGML=${all_GGML:-0}
    
    GGML_count=$((all_GGML - LM_GGML_count))
    
    # Ensure counts are non-negative
    if [ $ggml_count -lt 0 ]; then ggml_count=0; fi
    if [ $GGML_count -lt 0 ]; then GGML_count=0; fi
    if [ $lm_ggml_count -lt 0 ]; then lm_ggml_count=0; fi
    if [ $LM_GGML_count -lt 0 ]; then LM_GGML_count=0; fi
    
    # Calculate total prefixed and non-prefixed symbols
    total_prefixed=$((lm_ggml_count + LM_GGML_count))
    total_non_prefixed=$((ggml_count + GGML_count))
    
    # Skip processing if file already has mostly prefixes (95% threshold)
    if [ $total_prefixed -gt 0 ] && [ $total_non_prefixed -gt 0 ]; then
      ratio=$((total_prefixed * 100 / (total_prefixed + total_non_prefixed)))
      if [ $ratio -ge 95 ]; then
        echo "Skipping: $file (already has $ratio% prefixes)"
        continue
      fi
    elif [ $total_prefixed -gt 0 ] && [ $total_non_prefixed -eq 0 ]; then
      echo "Skipping: $file (already fully prefixed)"
      continue
    fi
    
    echo "Processing: $file"
    
    if [ "$OS" = "Darwin" ]; then
      # First remove any existing LM_ prefixes to avoid duplicates
      # This ensures consistent prefixing even if the file was processed before
      sed -i '' 's/LM_GGML_/GGML_/g' "$file"
      sed -i '' 's/lm_ggml_/ggml_/g' "$file"
      sed -i '' 's/LM_GGUF_/GGUF_/g' "$file"
      sed -i '' 's/lm_gguf_/gguf_/g' "$file"
      # Now add the LM_ prefixes
      sed -i '' 's/GGML_/LM_GGML_/g' "$file"
      sed -i '' 's/ggml_/lm_ggml_/g' "$file"
      sed -i '' 's/GGUF_/LM_GGUF_/g' "$file"
      sed -i '' 's/gguf_/lm_gguf_/g' "$file"
      sed -i '' 's/GGMLMetalClass/LMGGMLMetalClass/g' "$file"
      # Clean up any accidental multiple prefixes
      sed -i '' 's/LM_LM_LM_GGML_/LM_GGML_/g' "$file"
      sed -i '' 's/LM_LM_GGML_/LM_GGML_/g' "$file"
      sed -i '' 's/lm_lm_lm_ggml_/lm_ggml_/g' "$file"
      sed -i '' 's/lm_lm_ggml_/lm_ggml_/g' "$file"
      sed -i '' 's/LM_LM_LM_GGUF_/LM_GGUF_/g' "$file"
      sed -i '' 's/LM_LM_GGUF_/LM_GGUF_/g' "$file"
      sed -i '' 's/lm_lm_lm_gguf_/lm_gguf_/g' "$file"
      sed -i '' 's/lm_lm_gguf_/lm_gguf_/g' "$file"
    else
      # Linux version (no '' after -i)
      sed -i 's/LM_GGML_/GGML_/g' "$file"
      sed -i 's/lm_ggml_/ggml_/g' "$file"
      sed -i 's/LM_GGUF_/GGUF_/g' "$file"
      sed -i 's/lm_gguf_/gguf_/g' "$file"
      sed -i 's/GGML_/LM_GGML_/g' "$file"
      sed -i 's/ggml_/lm_ggml_/g' "$file"
      sed -i 's/GGUF_/LM_GGUF_/g' "$file"
      sed -i 's/gguf_/lm_gguf_/g' "$file"
      sed -i 's/GGMLMetalClass/LMGGMLMetalClass/g' "$file"
      sed -i 's/LM_LM_LM_GGML_/LM_GGML_/g' "$file"
      sed -i 's/LM_LM_GGML_/LM_GGML_/g' "$file"
      sed -i 's/lm_lm_lm_ggml_/lm_ggml_/g' "$file"
      sed -i 's/lm_lm_ggml_/lm_ggml_/g' "$file"
      sed -i 's/LM_LM_LM_GGUF_/LM_GGUF_/g' "$file"
      sed -i 's/LM_LM_GGUF_/LM_GGUF_/g' "$file"
      sed -i 's/lm_lm_lm_gguf_/lm_gguf_/g' "$file"
      sed -i 's/lm_lm_gguf_/lm_gguf_/g' "$file"
    fi
  else
    echo "Optional file not found, skipping: $file"
  fi
done

echo "Prefix processing completed successfully!"

echo ""
echo "Note: Files are processed intelligently based on their current prefix status."
echo "- If a file already has mostly (95%+) lm_ggml_/LM_GGML_ prefixes, it is skipped"
echo "- If a file has some prefixes but is inconsistent, it is processed to ensure consistency"
echo "- If a file has no prefixes, it gets them applied"

echo ""
echo "The script still uses the safe 'remove then reapply' approach for files that need processing,"
echo "which ensures consistent prefixing and avoids duplicate prefix issues."
