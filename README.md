# llama_mobile
Run quantized gguf model on mobile device locally
