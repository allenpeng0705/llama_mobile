#include <iostream>
#include <vector>
#include <string>
#include <filesystem>
#include <cstdlib>

#include "../llama_mobile_api.h"

namespace fs = std::filesystem;

std::vector<std::string> find_gguf_models(const std::string& directory) {
    std::vector<std::string> models;
    
    try {
        for (const auto& entry : fs::directory_iterator(directory)) {
            if (entry.is_regular_file() && entry.path().extension() == ".gguf") {
                models.push_back(entry.path().string());
            }
        }
    } catch (const fs::filesystem_error& e) {
        std::cerr << "Error accessing models directory: " << e.what() << std::endl;
    }
    
    return models;
}

std::string select_model() {
    std::string models_dir = "../../models";
    std::vector<std::string> models = find_gguf_models(models_dir);
    
    if (models.empty()) {
        std::cerr << "No .gguf models found in " << models_dir << std::endl;
        return "";
    }
    
    std::cout << "Available models:\n";
    for (size_t i = 0; i < models.size(); ++i) {
        std::cout << "[" << i + 1 << "] " << fs::path(models[i]).filename() << std::endl;
    }
    
    int choice;
    while (true) {
        std::cout << "\nSelect a model (1-" << models.size() << "): ";
        std::cin >> choice;
        
        if (choice >= 1 && choice <= static_cast<int>(models.size())) {
            break;
        }
        
        std::cout << "Invalid choice. Please try again.\n";
        std::cin.clear();
        std::cin.ignore(std::numeric_limits<std::streamsize>::max(), '\n');
    }
    
    return models[choice - 1];
}

int main(int argc, char* argv[]) {
    std::cout << "=== Llama Mobile Chat Example ===\n";
    
    // Select model - either from command line or interactive selection
    std::string model_path;
    if (argc > 1) {
        model_path = argv[1];
        std::cout << "Using model from command line: " << fs::path(model_path).filename() << std::endl;
    } else {
        model_path = select_model();
        if (model_path.empty()) {
            return 1;
        }
    }
    
    std::cout << "\nLoading model: " << fs::path(model_path).filename() << std::endl;
    
    // Initialize Llama Mobile
    llama_mobile_context_t ctx = llama_mobile_init_simple(
        model_path.c_str(),
        2048,    // n_ctx
        -1,      // n_gpu_layers (disable GPU)
        4,       // n_threads
        nullptr  // progress_callback
    );
    
    if (ctx == nullptr) {
        std::cerr << "Failed to initialize Llama Mobile" << std::endl;
        return 1;
    }
    
    std::cout << "Model loaded successfully!\n\n";
    
    // Add system prompt to establish assistant behavior
    const char* system_prompt = "You are a helpful assistant. Respond naturally to user queries.";
    llama_mobile_conversation_result_t system_result;
    int status = llama_mobile_generate_response_simple(
        ctx,
        system_prompt,
        0,  // No tokens needed for system prompt
        &system_result
    );
    
    if (status != 0) {
        std::cerr << "Failed to set system prompt: " << status << std::endl;
        llama_mobile_free(ctx);
        return 1;
    }
    
    if (system_result.text) {
        llama_mobile_free_string(system_result.text);  // Clean up system prompt response
    }
    
    std::string user_input;
    std::cout << "Type 'quit' or 'exit' to end the chat.\n";
    
    try {
        while (true) {
            // Get user input
            std::cout << "\nYou: ";
            
            // Clear any previous error flags
            std::cin.clear();
            
            // Ignore newline from previous input only if we're not at the end
            if (!std::cin.eof() && std::cin.peek() == '\n') {
                std::cin.ignore();
            }
            
            if (!std::getline(std::cin, user_input)) {
                // Handle EOF or input error
                if (std::cin.eof()) {
                    std::cout << "\n[EOF received, ending chat]" << std::endl;
                } else {
                    std::cerr << "\n[Error reading input]" << std::endl;
                }
                break;
            }
            
            if (user_input == "quit" || user_input == "exit") {
                break;
            }
            
            if (user_input.empty()) {
                std::cout << "Please enter a non-empty message." << std::endl;
                continue;
            }
            
            // Generate response
            std::cout << "\nAssistant: ";
            std::cout.flush();
            
            // Use the conversation API to maintain context
            llama_mobile_conversation_result_t conv_result;
            int status = llama_mobile_generate_response_simple(
                ctx,
                user_input.c_str(),
                200,  // max_tokens
                &conv_result
            );
            
            if (status == 0) {
                if (conv_result.text) {
                    std::cout << conv_result.text << std::endl;
                    // Free the response text
                    llama_mobile_free_string(conv_result.text);
                } else {
                    std::cerr << "[No response generated]" << std::endl;
                }
            } else {
                std::cerr << "[Failed to generate response (status: " << status << ")]" << std::endl;
            }
        }
    } catch (const std::exception& e) {
        std::cerr << "\n[Unexpected error: " << e.what() << "]" << std::endl;
    } catch (...) {
        std::cerr << "\n[Unknown error occurred]" << std::endl;
    }
    
    // Cleanup
    llama_mobile_free(ctx);
    
    std::cout << "\nChat ended. Goodbye!\n";
    
    return 0;
}
